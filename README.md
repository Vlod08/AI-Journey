# AI-Journey / Parcours en Intelligence Artificielle

Key/ Cle: 

- üõë : Not completed / Non compl√©t√©
- ‚ö†Ô∏è : To be revised / √Ä r√©viser
- ‚úÖ : Completed / Compl√©t√©
 

## Theory / Th√©orie :

### Matric Calculus / Calcul Matriciel : 
- Changement de base / Change of basis ‚úÖ
- Diagonalisation / Diagonalization ‚úÖ 
- Singular Value Decomposition / D√©composition en valeurs singuli√®res ‚úÖ  
- Kronecker Product and matrix derivatives / Produit de Kronecker et d√©riv√©es matricielles ‚úÖ
- [Lagrange Duality / Dualite Lgrangian](https://www.youtube.com/watch?v=thuYiebq1cE&t=1793s) ‚úÖ


### Probability and Satistics / Probabilit√© et Statistiques : 
- Lois discretes / Discrete distributions ‚úÖ
- Lois continues (Normale, Student, Khi-deux) / Continuous distributions (Normal, Student's t, Chi-square) ‚úÖ
- Lois multivariables / Multivariable distributions ‚úÖ



### Machine Learning :
* #### [CS229 Stanford course videos / Vid√©os de cours de Stanford](https://www.youtube.com/watch?v=het9HFqo1TQ&t=2143s) (Lectures completed 13/20) üõë
    * **Lecture 1 : Introduction and Examples** ‚úÖ
    * **Lecture 2 : Linear Regression and Gradient Descent** ‚úÖ
        - Cost Function (Least average squares) / Fonction de Co√ªt (M√©thode des moindres carr√©s)
        - Normal Equations / √âquations Normales
    * **Lecture 3 : Locally Weighted & Logistic Regression / R√©gression Locale Pond√©r√©e & Logistique** ‚úÖ
        - Weight function / Fonction de Poids
        - Sigmoid function and binary classification / Fonction Sigmo√Øde et Classification Binaire
        - Probabilistic Interpretation / Interpr√©tation probabiliste
        - Likelihood of parameters / Vraisemblance des Param√®tres
    * **Lecture 4 : Perceptron & Generalized Linear Model** ‚úÖ
        - Perceptron
        - exponential family / Famille Exponentielle
        - Generalised Linear Models (GLMs) 
        - Softmax Regression and Multi-class Classification 
    * **Lecture 5 : Gaussian Discriminant Analysis & Naive Bayes** ‚úÖ
        - Generative Learning Algorithms / Algorithmes d'Apprentissage G√©n√©ratif
        - Gaussian Dicriminant Analysis / Analyse Discriminante Gaussienne
        - Multivariate Gaussian Distribution / Distribution Gaussienne Multivari√©e
        - Singular Value Decomposition (SVD) / D√©composition en Valeurs Singuli√®res (SVD)
        - Introduction to Naive Bayes / Introduction au Na√Øf Bayes
    * **Lecture 6 : Support vector Machines** ‚úÖ
        - Naives Bayes & Spam classifier : Multivariate and Multinomial models / Na√Øf Bayes et Trieur de Spam : Mod√®les Multivari√©s et Multinomiaux
        - Laplace smoothing 
        - PROs and CONs of Generative & Discriminative Learning Algorithms / Avantages et Inconv√©nients des Algorithmes d'Apprentissage G√©n√©ratif et Discriminant
        - Introduction to Support Vector Machines (SVMs) / Introduction aux Machines √† Vecteurs de Support (SVMs)
        - Functional and Geometric margin / Marge Fonctionnelle et G√©om√©trique
    * **Lecture 7 : Kernels** ‚úÖ
        - Feature & Dimension augmentation / Augmentation de Caract√©ristiques et de Dimensions
        - Kernel trick and computation reduction / Noyaux et Optimisation
        - Lagrange Duality / Dualit√© de Lagrange
    * **Lecture 8 :  Data Splits, Models & Cross-Validation** ‚úÖ
        - Bias & Variance trade-off / Compromis Biais et Variance
        - Regularisation / R√©gularisation
        - Train/Devloppment/Test Split / S√©paration Entra√Ænement/D√©veloppement/Test
        - K-fold Cross Validation / Validation Crois√©e en K-Volets
    * **Lecture 9 : Approx/Estimation Error & ERM / Erreur d'Approximation/Estimation & Minimisation du Risque Empirique** ‚ö†Ô∏è
        - Formal definition of Bias and Variance / D√©finition formelle du Biais et de la Variance
        - Regularisation and Variance / R√©gularisation et Variance
        - Consistency and Efficiency of a model / Coh√©rence et Efficacit√© d'un mod√®le
        - Approximation/Empirical/Estimation/Irreductible/Generalisation Error / Erreur d'Approximation/Empirique/Estimation/Irr√©ductible/G√©n√©ralisation
        - Empirical Risk Minimisation / Minimisation du Risque Empirique
        - Uniform Convergence (Hoeffding's Inequality) / Convergence Uniforme (In√©galit√© de Hoeffding)
        - VC Dimension / Dimension VC
    * **Lecture 10 : Decision Trees and Ensemble Methods / Arbres de D√©cision et M√©thodes d'Ensemble** ‚úÖ
        - Miss-classification and Cross-entropy loss / Erreur de Classification et Perte d'Entropie Crois√©e
        - Information gain / Gain d'Information
        - Regression trees / Arbres de R√©gression
        - Pros (Fast, simple, Low Bias) and Cons (Bad additivity, High Variance) / Avantages (Rapide, Simple, Faible Biais) et Inconv√©nients (Mauvaise Additivit√©, Haute Variance)
        - Regularisation techniques for Decision trees / Techniques de R√©gularisation pour les Arbres de D√©cision
        - Runtime complexity / Complexit√© 
        - Ensembling Techniques(Bagging/Bootstrap sampling, Boosting, Random Forests, Stacking) / Techniques d'Ensemble (Bagging/√âchantillonnage Bootstrap, Boosting, For√™ts Al√©atoires, Stacking)
    * **Lecture 11 : Introduction to Neural Networks / Introduction aux R√©seaux de Neurones** ‚úÖ
        - Intro : Logistic Regression to Neural Networks / Introduction : De la R√©gression Logistique aux R√©seaux Neuronaux
        - Activation functions / Fonctions d'Activation
        - Architecture and parameters / Architecture et Param√®tres
        - Loss and cost function / Fonction de Perte et Co√ªt
        - Forward and Backward propogation equations / √âquations de la Propagation Avant et Arri√®re 
    * **Lecture 12 : Backprop & Improving Neural Networks R√©tropropagation & Am√©lioration des R√©seaux Neurones** ‚úÖ
        - Concrete example of back propogation / Exemple Concret de la R√©tropropagation
        - Improving Neural networks / Am√©liorer les R√©seaux Neurones :
            * advantages and disadvantages of different (ReLU, sigmoid, Tanh) / Avantages et Inconv√©nients des diff√©rentes Fonctions d'Activation (ReLU, Sigmo√Øde, Tanh)
            * Vanishing and Exploding  gradients / Gradients Disparus et Explosifs
            * Symmetry Problem / Probl√®me de Sym√©trie
            * Initialization Schemes / methodes d'Initialisation 
                1. Xavier Initialization 
                2. He Initialization
                3. np.random.rand(shape)*np.sqrt(1/n[L-1])
            * Normalization techniques / Techniques de Normalisation
            * Optimization / Optimisation
                1. Mini-Bathch Gradient Descent
                2. Momentum Algorithm    
    * **Lecture 13 : Debugging ML Models and Error Analysis / D√©bogage des Mod√®les ML et Analyse des Erreurs** ‚úÖ
        - Diagnostics for debugging learning algorithms / Diagnostics pour D√©boguer les Algorithmes d'Apprentissage  
            * Convergence of the optimization algoritm / Convergence de l'Algorithme d'Optimisation
            * Rightness of the objective function / Validit√© de la Fonction Objectif
        - Error analysis and ablative analysis / Analyse des Erreurs et Analyse Ablative
        - Premature statistical optimization  
  





     

* #### [stanford course notes / Notes de cours de Stanford](https://cs229.stanford.edu/syllabus-autumn2018.html) üõë

## Practical aspect and exercises / Aspect pratique et exercices :

### CS299-Stanford-Problem-Sets üõë: 

* #### [PS0-2018](./Problem-Sets%20Maths%20and%20Code/CS229-Machine-Learning-Stanford/PS0-2018): ‚úÖ
    - Linear Algebra & Matrix Calculus

* #### [PS1-2018](./Problem-Sets%20Maths%20and%20Code/CS229-Machine-Learning-Stanford/PS1-2018): ‚úÖ
    - Linear Classifiers (logistic regression and GDA)
    - Incomplete, Positive-Only Labels
    - Poisson Regression
    - Convexity of Generalized Linear Models
    - Locally Weighted Linear Regression
* #### [PS2-2018](./Problem-Sets%20Maths%20and%20Code/CS229-Machine-Learning-Stanford/PS2): üõë
* #### [PS3-2018](./Problem-Sets%20Maths%20and%20Code/CS229-Machine-Learning-Stanford/PS3): üõë
* #### [PS4-2018](./Problem-Sets%20Maths%20and%20Code/CS229-Machine-Learning-Stanford/PS4): üõë


### Coursera Courses / Cours Coursera :
* ####  Machine Learning Specialization / Sp√©cialisation en Apprentissage Automatique ‚úÖ [View Certificate](./Certificates/Coursera%20E4LSNKK33ML8.pdf) : 
    * **[Supervised Machine Learning: Regression and Classification (Beginners Level) / Apprentissage automatique supervis√© : R√©gression et Classification (Niveau d√©butant)](https://www.coursera.org/specializations/machine-learning-introduction) : 03-05-2024 - 05-07-2024** ‚úÖ  
        - Self implementation of cost & gradient descent functions for Linear regrssion & logistic classification / Impl√©mentation des fonctions de co√ªt et de descente
            de gradient pour la r√©gression lin√©aire et la classification logistique 
        - Feature Selection / S√©lection de caract√©ristiques
        - Feature Scaling and Normalisation / Mise √† l'√©chelle et Normalisation
        - Regularisation / R√©gularisation

    * **[Supervised Machine Learning: Advanced Learning Algorithms (Intermediate Level) / Apprentissage Supervis√© : Algorithmes d'Apprentissage Avanc√©s (Niveau Interm√©diaire)](https://www.coursera.org/learn/advanced-learning-algorithms) : 06-07-2024 08-08-2024**‚úÖ   
        - Introduction to Neural Networks / Introduction aux R√©seaux de Neurones
        - Tensor Flow
        - Softmax & Multiclass Classification / Softmax et Classification Multiclasse
        - Activation Functions (linear, sigmoid, relu, softmax, softmax) / Fonctions d'Activation (lin√©aire, sigmo√Øde, relu, softmax)
        - NN Layers (Dense and Convolutional) / Couches RN (Dense et Convolutionnelle) 
        - lossfunction (MeanErrorSquared, BinaryCrossEntropy,SparseCategoricalCrossEntropy, from_logits) / Fonctions de Perte (Erreur Quadratique Moyenne, Entropie
            Crois√©e Binaire, Entropie Crois√©e Cat√©gorique √âparse, from_logits)
        - Implementation of Neural Networks with TF / Impl√©mentation de R√©seaux de Neurones avec TF
        - Self implementation of Neural Networks with python (without using pre-built libraries) / Impl√©mentation autonome de r√©seaux de neurones avec Python (sans
            l'utilisation des biblioth√®ques pr√©-construites)
        - Optimizers (Adam) / Optimiseurs (Adam)
        - Forward and Backward Propogation / Propagation Avant et Arri√®re
        - Decision trees and Tree ensembles / Arbres de D√©cision et Ensembles d'Arbres
        - Entropy and Information Gain / Entropie et Gain d'Information
        - F1-Score
    
    * **[Unsupervised Learning, Recommenders, Reinforcement Learning (Intermediate Level) / Apprentissage Non Supervis√©, Syst√®mes de Recommandation, Apprentissage par Renforcement (Niveau Interm√©diaire)](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning) : 09-08-2024 08-09-2024**‚úÖ
        - Introduction to unsupervised learning / Introduction √† l'Apprentissage Non Supervis√©
        - Clustering & K-means algorithm 
        - Anomaly detection and Gaussian distribution / D√©tection d'Anomalies et Distribution Gaussienne
        - Recommendation systems / Syst√®mes de Recommandation
        - Collaborative and Content based filtering / Filtrage Collaboratif et Bas√© sur le Contenu
        - Reninforcement Learning & Reward function / Apprentissage par renforcement et fonction de r√©compense

* ####  IBM AI Engineering Professional Certificate / Certificat Professionnel en Ing√©nierie de l'IA d'IBM : 
    * **[Machine Learning with Python](https://www.coursera.org/learn/machine-learning-with-python) : 22-08-2024 active**üõë
        - Basic ML Algorithms (Linear & logistic regression, K-Nearest Neighbors, Regression Trees)


* ### Other Resources  
    * #### [ScikitLearn Tutorial (FreeCodeCamp)](https://www.youtube.com/watch?v=0B5eIE_1vpU&t=884s) ‚úÖ
        - Preprocessing / Pr√©traitement 
        - ML Pipelines / Pipelines ML
        - Cross Validation / Validation crois√©e

    